# -*- coding: utf-8 -*-
"""house_prises.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19LGV29Z6B7SQUZdPPebGvdCG3RT3CQY5
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

pd.set_option('display.max_columns', None)
pd.set_option('display.max_rows', 100)

train_df =  pd.read_csv('train.csv')
test_df = pd.read_csv('test.csv')

y_train = train_df['SalePrice'].copy()
train_features = train_df.drop(['Id', 'SalePrice'], axis=1)

test_features = test_df.drop('Id', axis=1)

all_df = pd.concat([train_features, test_features], ignore_index=True)

print(f"Shape of original train_df: {train_df.shape}")
print(f"Shape of original test_df: {test_df.shape}")
print(f"Shape of combined all_df: {all_df.shape}")

missing_values_all = all_df.isnull().sum().sort_values(ascending=False)
missing_percent_all = (all_df.isnull().sum() / len(all_df) * 100).sort_values(ascending=False)
missing_data_summary_all = pd.concat([missing_values_all, missing_percent_all], axis=1, keys=['Total Missing', 'Percent Missing'])

print("\nColumns with missing values (Combined Data - all_df):")
print(missing_data_summary_all[missing_data_summary_all['Total Missing'] > 0])

cols_fill_none_categorical = [
    'PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',
    'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',
    'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',
    'MasVnrType'
]

for col in cols_fill_none_categorical:
    all_df[col] = all_df[col].fillna('None')

print("Filled 'None' for categorical features. Checking a few values:")
print(all_df[['PoolQC', 'Alley', 'FireplaceQu', 'GarageType', 'BsmtQual', 'MasVnrType']].head())

cols_fill_zero_numerical = [
    'MasVnrArea',
    'GarageYrBlt',
    'GarageCars', 'GarageArea',
    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',
    'BsmtFullBath', 'BsmtHalfBath'
]

for col in cols_fill_zero_numerical:
    all_df[col] = all_df[col].fillna(0)

print("\nFilled 0 for related numerical features. Checking a few values:")
print(all_df[all_df['GarageType'] == 'None'][['GarageType', 'GarageCars', 'GarageArea', 'GarageYrBlt']].head())
print(all_df[all_df['BsmtQual'] == 'None'][['BsmtQual', 'TotalBsmtSF', 'BsmtFullBath']].head())

missing_values_after_group1_2 = all_df.isnull().sum().sort_values(ascending=False)
missing_percent_after_group1_2 = (all_df.isnull().sum() / len(all_df) * 100).sort_values(ascending=False)
missing_summary_after_group1_2 = pd.concat(
    [missing_values_after_group1_2, missing_percent_after_group1_2],
    axis=1, keys=['Total Missing', 'Percent Missing']
)

print("\nColumns with missing values (After Group 1 & 2 Imputation):")
print(missing_summary_after_group1_2[missing_summary_after_group1_2['Total Missing'] > 0])

print(f"Missing LotFrontage before imputation: {all_df['LotFrontage'].isnull().sum()}")

all_df['LotFrontage'] = all_df.groupby('Neighborhood')['LotFrontage'] \
                              .transform(lambda x: x.fillna(x.median()))

remaining_lotfrontage_na = all_df['LotFrontage'].isnull().sum()
if remaining_lotfrontage_na > 0:
    print(f"Still {remaining_lotfrontage_na} missing LotFrontage values after neighborhood median imputation.")
    global_median_lotfrontage = all_df['LotFrontage'].median()
    all_df['LotFrontage'] = all_df['LotFrontage'].fillna(global_median_lotfrontage)
    print(f"Filled remaining with global median: {global_median_lotfrontage}")

print(f"Missing LotFrontage after imputation: {all_df['LotFrontage'].isnull().sum()}")

cols_fill_mode_categorical = [
    'MSZoning', 'Utilities', 'Functional', 'Exterior1st',
    'Exterior2nd', 'Electrical', 'SaleType', 'KitchenQual'
]

for col in cols_fill_mode_categorical:
    if col in all_df.columns and all_df[col].isnull().any():
        mode_val = all_df[col].mode()[0]
        all_df[col] = all_df[col].fillna(mode_val)
        print(f"Filled NaN in '{col}' with mode: '{mode_val}'. Missing now: {all_df[col].isnull().sum()}")
    elif col not in all_df.columns:
        print(f"Warning: Column '{col}' not found in DataFrame.")
print(f"\nMissing MSZoning after imputation: {all_df['MSZoning'].isnull().sum()}")

final_missing_summary = all_df.isnull().sum().sort_values(ascending=False)
print("\nFinal check for missing values in all_df:")
print(final_missing_summary[final_missing_summary > 0])

if final_missing_summary.sum() == 0:
    print("\nCongratulations! No more missing values in all_df.")
else:
    print("\nWarning! Some missing values still remain. Please review.")

print(f"Data type of MSSubClass before conversion: {all_df['MSSubClass'].dtype}")
all_df['MSSubClass'] = all_df['MSSubClass'].astype(str)
print(f"Data type of MSSubClass after conversion: {all_df['MSSubClass'].dtype}")
print(f"Unique values in MSSubClass after conversion: {all_df['MSSubClass'].unique()[:5]}...")

print("\nData types of all columns in all_df:")
print(all_df.dtypes.value_counts())
print("\nFirst few columns with their types:")
print(all_df.dtypes.head(10))
print("\nLast few columns with their types:")
print(all_df.dtypes.tail(10))

n_train = train_df.shape[0]
print(f"Number of rows in original training set: {n_train}")
train_eda_df = all_df[:n_train].copy()
test_eda_df = all_df[n_train:].copy()

train_eda_df['SalePrice'] = y_train

print(f"Shape of train_eda_df: {train_eda_df.shape}")
print(f"Shape of test_eda_df: {test_eda_df.shape}")

import scipy.stats as stats

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.histplot(train_eda_df['SalePrice'], kde=True, bins=50)
plt.title('Distribution of SalePrice')
plt.xlabel('SalePrice')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
stats.probplot(train_eda_df['SalePrice'], plot=plt)
plt.title('Q-Q Plot of SalePrice')

plt.tight_layout()
plt.show()


skewness = train_eda_df['SalePrice'].skew()
kurtosis = train_eda_df['SalePrice'].kurt()
print(f"Skewness of SalePrice: {skewness:.2f}")
print(f"Kurtosis of SalePrice: {kurtosis:.2f}")

train_eda_df['SalePrice_Log'] = np.log(train_eda_df['SalePrice'])

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.histplot(train_eda_df['SalePrice_Log'], kde=True, bins=50, color='green')
plt.title('Distribution of Log-Transformed SalePrice')
plt.xlabel('Log(SalePrice)')
plt.ylabel('Frequency')

plt.subplot(1, 2, 2)
stats.probplot(train_eda_df['SalePrice_Log'], plot=plt)
plt.title('Q-Q Plot of Log-Transformed SalePrice')

plt.tight_layout()
plt.show()

log_skewness = train_eda_df['SalePrice_Log'].skew()
log_kurtosis = train_eda_df['SalePrice_Log'].kurt()
print(f"Skewness of Log-Transformed SalePrice: {log_skewness:.2f}")
print(f"Kurtosis of Log-Transformed SalePrice: {log_kurtosis:.2f}")

if 'SalePrice_Log' not in train_eda_df.columns:
    train_eda_df['SalePrice_Log'] = np.log(train_eda_df['SalePrice'])

numerical_features = train_eda_df.select_dtypes(include=np.number)

correlation_matrix = numerical_features.corr()

sale_price_log_corr = correlation_matrix['SalePrice_Log'].sort_values(ascending=False)

print("Correlation of numerical features with SalePrice_Log (descending):")
print(sale_price_log_corr)
plt.figure(figsize=(8, 10))
top_corr = sale_price_log_corr.drop(['SalePrice_Log', 'SalePrice']).head(15)
sns.barplot(x=top_corr.values, y=top_corr.index, palette="viridis")
plt.title('Top 15 Numerical Features Correlated with Log(SalePrice)')
plt.xlabel('Correlation Coefficient')
plt.ylabel('Features')
plt.tight_layout()
plt.show()

scatter_features = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF', 'YearBuilt']

plt.figure(figsize=(15, 10))
for i, feature in enumerate(scatter_features):
    plt.subplot(2, 3, i + 1)
    sns.scatterplot(x=train_eda_df[feature], y=train_eda_df['SalePrice_Log'])
    plt.title(f'{feature} vs. Log(SalePrice)')
    plt.xlabel(feature)
    plt.ylabel('Log(SalePrice)')

plt.tight_layout()
plt.show()

plt.figure(figsize=(18, 15)) # May need to adjust size
sns.heatmap(correlation_matrix, annot=False, cmap='coolwarm', fmt=".1f", linewidths=.5)
plt.title('Correlation Matrix of Numerical Features')
plt.show()
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))
corr_unstacked = correlation_matrix.mask(mask).stack().sort_values(ascending=False)
high_correlations = corr_unstacked[ (abs(corr_unstacked) > 0.7) & (abs(corr_unstacked) < 1.0) ]

print("\nPairs of features with high correlation (> 0.7 or < -0.7):")
print(high_correlations)

categorical_eda_features = ['Neighborhood', 'HouseStyle', 'BsmtQual', 'KitchenQual', 'SaleCondition']
if 'SalePrice_Log' not in train_eda_df.columns:
    train_eda_df['SalePrice_Log'] = np.log(train_eda_df['SalePrice'])


plt.figure(figsize=(18, 15))
for i, feature in enumerate(categorical_eda_features):
    if train_eda_df[feature].nunique() > 5:
        order = train_eda_df.groupby(feature)['SalePrice_Log'].median().sort_values().index
        plt.subplot(3, 2, i + 1)
        sns.boxplot(x=feature, y='SalePrice_Log', data=train_eda_df, order=order, palette="Set3")
        plt.xticks(rotation=45, ha='right')
    else:
        plt.subplot(3, 2, i + 1)
        sns.boxplot(x=feature, y='SalePrice_Log', data=train_eda_df, palette="Set3")

    plt.title(f'{feature} vs. Log(SalePrice)')
    plt.xlabel(feature)
    plt.ylabel('Log(SalePrice)')

plt.tight_layout()
plt.show()

print(f"Original train_eda_df shape: {train_eda_df.shape}")
print(f"Original y_train (SalePrice_Log) shape: {train_eda_df['SalePrice_Log'].shape}") # Assuming y_train is SalePrice_Log

outlier_indices_train = train_eda_df[(train_eda_df['GrLivArea'] > 4000) & (train_eda_df['SalePrice_Log'] < 12.5)].index
print(f"Indices of GrLivArea outliers in train_eda_df: {outlier_indices_train}")

if not outlier_indices_train.empty:
    train_eda_df = train_eda_df.drop(outlier_indices_train)
    y_train_log_fe = train_eda_df['SalePrice_Log'].copy()
    print(f"Shape of train_eda_df after removing GrLivArea outliers: {train_eda_df.shape}")
    print(f"Shape of y_train_log_fe after removing GrLivArea outliers: {y_train_log_fe.shape}")
else:
    print("No GrLivArea outliers found with the specified criteria, or already handled.")
    y_train_log_fe = train_eda_df['SalePrice_Log'].copy() # Keep existing target

if not outlier_indices_train.empty:
    all_df_fe = all_df.drop(outlier_indices_train).reset_index(drop=True)
    print(f"Shape of all_df_fe after removing outliers: {all_df_fe.shape}")
else:
    all_df_fe = all_df.copy()
    print("No GrLivArea outliers to remove from all_df, or already handled.")

n_train_fe = train_eda_df.shape[0]
print(f"New n_train_fe (training set size after outlier removal): {n_train_fe}")

# Make sure YrSold, YearBuilt, YearRemodAdd, GarageYrBlt are numeric
for col in ['YrSold', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:
    all_df_fe[col] = pd.to_numeric(all_df_fe[col], errors='coerce') # Coerce will turn errors into NaT/NaN

all_df_fe['HouseAge'] = all_df_fe['YrSold'] - all_df_fe['YearBuilt']
all_df_fe['RemodelAge'] = all_df_fe['YrSold'] - all_df_fe['YearRemodAdd']

all_df_fe['GarageAge'] = all_df_fe['YrSold'] - all_df_fe['GarageYrBlt']
all_df_fe.loc[all_df_fe['GarageYrBlt'] == 0, 'GarageAge'] = 0

print(f"Min HouseAge: {all_df_fe['HouseAge'].min()}")
print(f"Min RemodelAge: {all_df_fe['RemodelAge'].min()}")
print(f"Min GarageAge: {all_df_fe['GarageAge'].min()}")
print(f"Number of rows where YearRemodAdd < YearBuilt: { (all_df_fe['YearRemodAdd'] < all_df_fe['YearBuilt']).sum() }")
print("\nSample of new Age features:")
print(all_df_fe[['YrSold', 'YearBuilt', 'HouseAge', 'YearRemodAdd', 'RemodelAge', 'GarageYrBlt', 'GarageAge']].head())

print(f"Shape of all_df_fe before age corrections: {all_df_fe.shape}")
for col in ['YrSold', 'YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:
    all_df_fe[col] = pd.to_numeric(all_df_fe[col], errors='coerce')

condition_remod_before_built = all_df_fe['YearRemodAdd'] < all_df_fe['YearBuilt']
print(f"Number of rows where YearRemodAdd < YearBuilt originally: {condition_remod_before_built.sum()}")
all_df_fe.loc[condition_remod_before_built, 'YearRemodAdd'] = all_df_fe.loc[condition_remod_before_built, 'YearBuilt']
print(f"Number of rows where YearRemodAdd < YearBuilt after correction: {(all_df_fe['YearRemodAdd'] < all_df_fe['YearBuilt']).sum()}")


max_yr_sold = all_df_fe['YrSold'].max()
condition_garage_future = all_df_fe['GarageYrBlt'] > all_df_fe['YrSold']
condition_garage_absurd_future = all_df_fe['GarageYrBlt'] > max_yr_sold + 10
print(f"Number of rows where GarageYrBlt > YrSold: {condition_garage_future.sum()}")
print(f"Number of rows where GarageYrBlt > {max_yr_sold + 10}: {condition_garage_absurd_future.sum()}")

all_df_fe.loc[condition_garage_future, 'GarageYrBlt'] = all_df_fe.loc[condition_garage_future, 'YrSold']
all_df_fe.loc[condition_garage_absurd_future, 'GarageYrBlt'] = all_df_fe.loc[condition_garage_absurd_future, 'YrSold']


all_df_fe['HouseAge'] = all_df_fe['YrSold'] - all_df_fe['YearBuilt']
all_df_fe['RemodelAge'] = all_df_fe['YrSold'] - all_df_fe['YearRemodAdd']
all_df_fe['GarageAge'] = all_df_fe['YrSold'] - all_df_fe['GarageYrBlt']

all_df_fe.loc[all_df_fe['GarageYrBlt'] == 0, 'GarageAge'] = 0

all_df_fe.loc[all_df_fe['HouseAge'] < 0, 'HouseAge'] = 0
all_df_fe.loc[all_df_fe['RemodelAge'] < 0, 'RemodelAge'] = 0
all_df_fe.loc[all_df_fe['GarageAge'] < 0, 'GarageAge'] = 0

print(f"\nMin HouseAge after correction: {all_df_fe['HouseAge'].min()}")
print(f"Min RemodelAge after correction: {all_df_fe['RemodelAge'].min()}")
print(f"Min GarageAge after correction: {all_df_fe['GarageAge'].min()}")

print("\nSample of new Age features after corrections:")
print(all_df_fe[['YrSold', 'YearBuilt', 'HouseAge', 'YearRemodAdd', 'RemodelAge', 'GarageYrBlt', 'GarageAge']].head())
condition_garage_future_after_corr = all_df_fe['GarageYrBlt'] > all_df_fe['YrSold']
print(f"Number of rows where GarageYrBlt > YrSold AFTER correction: {condition_garage_future_after_corr.sum()}")

sf_components = ['TotalBsmtSF', 'GrLivArea', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch']
bath_components = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath']

for col in sf_components + bath_components:
    all_df_fe[col] = pd.to_numeric(all_df_fe[col], errors='coerce').fillna(0)

# Total Square Footage
all_df_fe['TotalSF'] = all_df_fe['TotalBsmtSF'] + all_df_fe['GrLivArea']

# Total Porch Square Footage
all_df_fe['TotalPorchSF'] = (all_df_fe['OpenPorchSF'] +
                             all_df_fe['EnclosedPorch'] +
                             all_df_fe['3SsnPorch'] +
                             all_df_fe['ScreenPorch'])

# Total Number of Bathrooms
all_df_fe['TotalBath'] = (all_df_fe['BsmtFullBath'] +
                          0.5 * all_df_fe['BsmtHalfBath'] +
                          all_df_fe['FullBath'] +
                          0.5 * all_df_fe['HalfBath'])


print("\nSample of new Total SF / Count features:")
print(all_df_fe[['TotalSF', 'GrLivArea', 'TotalBsmtSF', 'TotalPorchSF', 'OpenPorchSF', 'TotalBath', 'FullBath', 'HalfBath']].head())

print(f"\nShape of all_df_fe after adding Total SF/Count features: {all_df_fe.shape}")
# Expected shape: 82 (previous) + 3 (new) = 85

key_numeric_for_poly = ['OverallQual', 'GrLivArea', 'TotalSF', 'HouseAge', 'GarageCars', 'TotalBath']

for feature in key_numeric_for_poly:
    if feature in all_df_fe.columns:
        all_df_fe[f'{feature}_sq'] = all_df_fe[feature] ** 2
    else:
        print(f"Warning: Feature '{feature}' not found for polynomial creation.")

print("\nSample of new Polynomial features (showing original and squared):")
sample_poly_cols = []
for f in key_numeric_for_poly:
    if f in all_df_fe.columns: sample_poly_cols.append(f)
    if f'{f}_sq' in all_df_fe.columns: sample_poly_cols.append(f'{f}_sq')

print(all_df_fe[sample_poly_cols].head())

print(f"\nShape of all_df_fe after adding polynomial features: {all_df_fe.shape}")
# Expected shape: 85 (previous) + 6 (new squared features) = 91

# Identify numerical features (excluding our already transformed target)
numerical_cols_in_all_df_fe = all_df_fe.select_dtypes(include=np.number).columns

# Calculate skewness for these numerical features
skewness_df = all_df_fe[numerical_cols_in_all_df_fe].apply(lambda x: x.skew()).sort_values(ascending=False)

print("Skewness of numerical features in all_df_fe:")
print(skewness_df.head(15))
print("---")
print(skewness_df.tail(15))
skew_threshold = 0.75
highly_skewed_cols = skewness_df[abs(skewness_df) > skew_threshold].index

print(f"\nNumerical features with absolute skewness > {skew_threshold}:")
print(highly_skewed_cols)
print(f"Number of highly skewed features: {len(highly_skewed_cols)}")

for col in highly_skewed_cols:
    if col in all_df_fe.columns:
        all_df_fe[col] = np.log1p(all_df_fe[col])
        print(f"Applied log1p to: {col}, new skewness: {all_df_fe[col].skew():.4f}")

print("\nDone with log transformation of skewed numerical features.")

print(f"Shape of all_df_fe before one-hot encoding: {all_df_fe.shape}")
print(f"Number of object columns before encoding: {len(all_df_fe.select_dtypes(include='object').columns)}")

all_df_encoded = pd.get_dummies(all_df_fe, dummy_na=False)

print(f"Shape of all_df_encoded after one-hot encoding: {all_df_encoded.shape}")
print(f"Number of object columns after encoding: {len(all_df_encoded.select_dtypes(include='object').columns)}")
print(f"Number of new columns created by get_dummies: {all_df_encoded.shape[1] - all_df_fe.shape[1]}")

original_mszoning_cols = [col for col in all_df_encoded.columns if 'MSZoning' in col]
print("\nSample of MSZoning after one-hot encoding:")
print(all_df_encoded[original_mszoning_cols].head())

print(f"Number of training samples (n_train_fe): {n_train_fe}")

X = all_df_encoded.copy()

X_train_processed = X[:n_train_fe]
X_test_processed = X[n_train_fe:]

y_train_final = y_train_log_fe.copy()

print(f"\nShape of X_train_processed: {X_train_processed.shape}")
print(f"Shape of y_train_final: {y_train_final.shape}")
print(f"Shape of X_test_processed: {X_test_processed.shape}")

# Verify consistency
if X_train_processed.shape[0] == y_train_final.shape[0]:
    print("\nTrain features and target have consistent number of samples. Good!")
else:
    print("\nError! Mismatch in number of samples between X_train_processed and y_train_final.")

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train_scaled = scaler.fit_transform(X_train_processed)
X_test_scaled = scaler.transform(X_test_processed)

X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train_processed.columns, index=X_train_processed.index)
X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test_processed.columns, index=X_test_processed.index)

print("\nData scaling complete.")
print("Sample of scaled training data (first 5 rows, first 5 columns):")
print(X_train_scaled_df.iloc[:5, :5])

print("\nMean of first 5 columns in scaled training data (should be close to 0):")
print(X_train_scaled_df.iloc[:, :5].mean())

print("\nStd dev of first 5 columns in scaled training data (should be close to 1):")
print(X_train_scaled_df.iloc[:, :5].std())

from sklearn.linear_model import Ridge
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import mean_squared_error
import numpy as np

X_train_np = X_train_scaled_df.to_numpy()
y_train_np = y_train_final.to_numpy()
X_test_np = X_test_scaled_df.to_numpy()

ridge_model = Ridge(alpha=1.0, random_state=42)
kf = KFold(n_splits=5, shuffle=True, random_state=42)

cv_scores_neg_mse = cross_val_score(ridge_model, X_train_np, y_train_np,
                                    cv=kf, scoring='neg_mean_squared_error')
cv_scores_rmse = np.sqrt(-cv_scores_neg_mse)

print("Ridge Regression Cross-Validation RMSE scores:")
print(cv_scores_rmse)
print(f"Mean CV RMSE: {cv_scores_rmse.mean():.4f}")
print(f"Std Dev CV RMSE: {cv_scores_rmse.std():.4f}")


ridge_model.fit(X_train_np, y_train_np)
print("\nRidge model trained on full training data.")

import lightgbm as lgb
import pandas as pd
import numpy as np
X_train_np = X_train_scaled_df.to_numpy()
y_train_np = y_train_final.to_numpy()

params_for_cv = {
    'objective': 'regression_l1',
    'metric': 'rmse',
    'n_estimators': 2000,
    'learning_rate': 0.01,
    'num_leaves': 31,
    'max_depth': -1,
    'min_child_samples': 20,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'random_state': 42,
    'n_jobs': -1,
    'verbose': -1,
    'boosting_type': 'gbdt',
}


lgb_train = lgb.Dataset(X_train_np, y_train_np)

cv_results = lgb.cv(
    params_for_cv,
    lgb_train,
    num_boost_round=params_for_cv['n_estimators'],
    nfold=5,
    stratified=False,
    shuffle=True,
    seed=42,
    callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=-1),
               lgb.log_evaluation(period=0)]
)

num_rounds_run = len(cv_results['valid rmse-mean'])
best_rmse_mean = cv_results['valid rmse-mean'][-1]
best_rmse_stdv = cv_results['valid rmse-stdv'][-1]

print(f"LightGBM CV ran for {num_rounds_run} rounds due to early stopping.")
print(f"Mean CV RMSE (LGBM) at best round: {best_rmse_mean:.4f}")
print(f"Std Dev CV RMSE (LGBM) at best round: {best_rmse_stdv:.4f}")


params_for_final_model = params_for_cv.copy()
params_for_final_model['n_estimators'] = num_rounds_run


final_lgbm_model = lgb.LGBMRegressor(**params_for_final_model)
final_lgbm_model.fit(X_train_np, y_train_np)

print("\nLightGBM model trained on full training data with optimal n_estimators from CV.")

!pip install lightgbm

log_test_predictions_lgbm = final_lgbm_model.predict(X_test_np)

actual_test_predictions_lgbm = np.exp(log_test_predictions_lgbm)
submission_df_lgbm = pd.DataFrame()
submission_df_lgbm['Id'] = test_df['Id']
submission_df_lgbm['SalePrice'] = actual_test_predictions_lgbm

# Save the submission file
submission_filename_lgbm = 'submission_lightgbm.csv'
submission_df_lgbm.to_csv(submission_filename_lgbm, index=False)

print(f"\nSubmission file '{submission_filename_lgbm}' created successfully.")
print("First 5 rows of the submission file:")
print(submission_df_lgbm.head())

from google.colab import files
files.download("submission_lightgbm.csv")

